{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dpbVFu2LNXr0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data_loader(data_dir, batch_size, random_seed, valid_size, shuffle = True, test = False):\n",
        "\n",
        "  transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "  train_dataset = datasets.MNIST(root= data_dir, train=True, download=True, transform=transforms)\n",
        "  valid_dataset = datasets.MNIST(root= data_dir, train=True, download=True, transform=transforms)\n",
        "  test_dataset = datasets.MNIST( root= data_dir, train=False, download=True, transform=transforms)\n",
        "\n",
        "  num_train = len(train_dataset)\n",
        "\n",
        "\n",
        "  indices = list(range(num_train))\n",
        "\n",
        "  split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "  if shuffle:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "  train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "  valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader\n"
      ],
      "metadata": {
        "id": "Rs2ufqsqQMwv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "03lJEiPVRZRX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "UUMRo0YtSC00"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, num_epochs, criterion, optimizer, device):\n",
        "\n",
        "  total_steps = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # move images and labels to device\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ],
      "metadata": {
        "id": "Abg1H9TmSP79"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(mode, valid_loader, device):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in valid_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      del labels, images, outputs\n",
        "    print('Accuracy of the network on the {} validation images: {:.2f} %'.format(5000, 100 * correct / total))"
      ],
      "metadata": {
        "id": "RmkPZ51cS_RY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "id": "SeKnF4ZmTpJf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_size = 0.1\n",
        "shuffle = True\n",
        "random_seed = 412\n",
        "batch_size = 64\n",
        "data_dir = './data'\n",
        "device = 'cpu'\n",
        "\n",
        "# Hyperparameters\n",
        "max_lr = 0.00001\n",
        "weight_decay = 0.005\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "\n",
        "# dataset definition\n",
        "train_loader, valid_loader, test_loader = build_data_loader(data_dir, batch_size, random_seed, valid_size, shuffle = True, test = False)\n",
        "\n",
        "# Model definition\n",
        "model = CNN()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optimizer(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "\n",
        "print(count_parameters(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a4gZpvPOK1T",
        "outputId": "5665cf60-a6fd-4616-dbd4-8ea2597bb68c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1630090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train the model\n",
        "train(model, train_loader, num_epochs, criterion, optimizer, device)\n",
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6SKV_KO4Ut",
        "outputId": "44993dad-ba5e-4056-b637-df0649672538"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.6461\n",
            "Epoch [2/5], Loss: 0.4391\n",
            "Epoch [3/5], Loss: 0.3048\n",
            "Epoch [4/5], Loss: 0.2326\n",
            "Epoch [5/5], Loss: 0.2475\n",
            "Accuracy of the network on the 10000 test images: 93.65 %\n",
            "CPU times: user 6min 30s, sys: 45.2 s, total: 7min 16s\n",
            "Wall time: 7min 18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prof = profile(activities=[ProfilerActivity.CPU])\n",
        "\n",
        "input_sample, _ = next(iter(train_loader))\n",
        "\n",
        "prof.start()\n",
        "model(input_sample)\n",
        "prof.stop()\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwK18umRSLUb",
        "outputId": "31f8c59c-7923-4cab-c738-3a412e4c7857"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    aten::max_pool2d_with_indices        46.36%      25.634ms        46.36%      25.634ms      12.817ms             2  \n",
            "         aten::mkldnn_convolution        37.24%      20.590ms        37.39%      20.670ms      10.335ms             2  \n",
            "                      aten::addmm        12.27%       6.782ms        12.38%       6.847ms       3.424ms             2  \n",
            "                  aten::clamp_min         2.68%       1.484ms         2.68%       1.484ms     742.000us             2  \n",
            "                       aten::relu         0.42%     234.000us         3.11%       1.718ms     859.000us             2  \n",
            "                     aten::conv2d         0.24%     131.000us        37.89%      20.946ms      10.473ms             2  \n",
            "                aten::convolution         0.17%      95.000us        37.65%      20.815ms      10.408ms             2  \n",
            "                     aten::linear         0.10%      53.000us        12.57%       6.952ms       3.476ms             2  \n",
            "                      aten::copy_         0.09%      52.000us         0.09%      52.000us      26.000us             2  \n",
            "               aten::_convolution         0.09%      50.000us        37.48%      20.720ms      10.360ms             2  \n",
            "                      aten::empty         0.09%      50.000us         0.09%      50.000us      12.500us             4  \n",
            "                          aten::t         0.06%      31.000us         0.09%      52.000us      26.000us             2  \n",
            "                aten::as_strided_         0.04%      23.000us         0.04%      23.000us      11.500us             2  \n",
            "                       aten::view         0.03%      17.000us         0.03%      17.000us      17.000us             1  \n",
            "                 aten::max_pool2d         0.03%      16.000us        46.39%      25.650ms      12.825ms             2  \n",
            "                  aten::transpose         0.02%      13.000us         0.04%      21.000us      10.500us             2  \n",
            "                 aten::as_strided         0.02%      11.000us         0.02%      11.000us       2.750us             4  \n",
            "                     aten::expand         0.02%      10.000us         0.02%      13.000us       6.500us             2  \n",
            "                    aten::resize_         0.01%       7.000us         0.01%       7.000us       3.500us             2  \n",
            "                    aten::reshape         0.01%       5.000us         0.04%      22.000us      22.000us             1  \n",
            "               aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             4  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 55.288ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7XbH97nWQpe",
        "outputId": "dc650f73-ebab-4712-c902-1d10d335088f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_names = ['input']\n",
        "output_names = ['output']\n",
        "torch.onnx.export(model, input_sample, \"cnn.onnx\", input_names=input_names, output_names=output_names, export_params=True)"
      ],
      "metadata": {
        "id": "S3djPHt1T4Tu"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}